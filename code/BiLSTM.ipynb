{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHnbGR5wpCL8"
   },
   "source": [
    "# CSE 291 Assignment 2 BiLSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs2O4920pCob"
   },
   "source": [
    "## Download Data/Eval Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hmfarI0hpHj6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-31 05:08:32--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7502 (7.3K) [text/plain]\n",
      "Saving to: ‘conlleval.py.3’\n",
      "\n",
      "conlleval.py.3      100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-05-31 05:08:32 (55.5 MB/s) - ‘conlleval.py.3’ saved [7502/7502]\n",
      "\n",
      "--2021-05-31 05:08:32--  https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/train.data.quad\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 745734 (728K) [text/plain]\n",
      "Saving to: ‘train.data.quad.2’\n",
      "\n",
      "train.data.quad.2   100%[===================>] 728.26K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-05-31 05:08:32 (11.9 MB/s) - ‘train.data.quad.2’ saved [745734/745734]\n",
      "\n",
      "--2021-05-31 05:08:32--  https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/dev.data.quad\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 179141 (175K) [text/plain]\n",
      "Saving to: ‘dev.data.quad.2’\n",
      "\n",
      "dev.data.quad.2     100%[===================>] 174.94K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-05-31 05:08:32 (7.67 MB/s) - ‘dev.data.quad.2’ saved [179141/179141]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
    "!wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/train.data.quad\n",
    "!wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/dev.data.quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0CMvXrmwpNCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import conlleval\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pdb \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "torch.manual_seed(291)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOBmqHytpTGs"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GKfmSZs8pPBV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word vocab: 3947 symbols.\n",
      "Train label vocab: 8 symbols: ['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n",
      "Train data: 3420 sentences.\n",
      "Valid data: 800\n",
      "Pusan 0000 0000 0000 0000 0000 0000\n",
      "I-ORG O O O O O O\n",
      "Earlier this month , <unk> denied a Kabul government statement that the two sides had agreed to a ceasefire in the north .\n",
      "O O O O I-PER O O I-LOC O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = 'train.data.quad'\n",
    "VALID_DATA = 'dev.data.quad'\n",
    "UNK = '<unk>'\n",
    "PAD = '<pad>'\n",
    "START_TAG = \"<start>\"  # you can add this explicitly or use it implicitly in your CRF layer\n",
    "STOP_TAG = \"<stop>\"    # you can add this explicitly or use it implicitly in your CRF layer\n",
    "\n",
    "\n",
    "def read_conll_sentence(path):\n",
    "    \"\"\" Read a CONLL-format sentence into vocab objects\n",
    "    Args:\n",
    "        :param path: path to CONLL-format data file\n",
    "        :param word_vocab: Vocabulary object for source\n",
    "        :param label_vocab: Vocabulary object for target\n",
    "    \"\"\"\n",
    "    sent = [[], []]\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line:\n",
    "                # replace numbers with 0000\n",
    "                word = line[0]\n",
    "                word = '0000' if word.isnumeric() else word\n",
    "                sent[0].append(word)\n",
    "                sent[1].append(line[3])\n",
    "            else:\n",
    "                yield sent[0], sent[1]\n",
    "                sent = [[], []]\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, word_vocab, label_vocab):\n",
    "    dataset = [\n",
    "      [\n",
    "        torch.tensor([word_vocab.stoi[word] for word in sent[0]], dtype=torch.long),\n",
    "        torch.tensor([label_vocab.stoi[label] for label in sent[1]], dtype=torch.long),\n",
    "      ]\n",
    "      for sent in dataset\n",
    "    ]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# load a list of sentences, where each word in the list is a tuple containing the word and the label\n",
    "train_data = list(read_conll_sentence(TRAIN_DATA))\n",
    "train_word_counter = Counter([word for sent in train_data for word in sent[0]])\n",
    "train_label_counter = Counter([label for sent in train_data for label in sent[1]])\n",
    "word_vocab = Vocab(train_word_counter, specials=(UNK, PAD), min_freq=2)\n",
    "label_vocab = Vocab(train_label_counter, specials=(), min_freq=1)\n",
    "train_data = prepare_dataset(train_data, word_vocab, label_vocab)\n",
    "print('Train word vocab:', len(word_vocab), 'symbols.')\n",
    "print('Train label vocab:', len(label_vocab), f'symbols: {list(label_vocab.stoi.keys())}')\n",
    "valid_data = list(read_conll_sentence(VALID_DATA))\n",
    "valid_data = prepare_dataset(valid_data, word_vocab, label_vocab)\n",
    "print('Train data:', len(train_data), 'sentences.')\n",
    "print('Valid data:', len(valid_data))\n",
    "\n",
    "print(' '.join([word_vocab.itos[i.item()] for i in train_data[0][0]]))\n",
    "print(' '.join([label_vocab.itos[i.item()] for i in train_data[0][1]]))\n",
    "\n",
    "print(' '.join([word_vocab.itos[i.item()] for i in valid_data[1][0]]))\n",
    "print(' '.join([label_vocab.itos[i.item()] for i in valid_data[1][1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNNmZx_Uqy7q"
   },
   "source": [
    "## BiLSTMTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a5nVIM_Eq1ZU"
   },
   "outputs": [],
   "source": [
    "# Starter code implementing a BiLSTM Tagger\n",
    "# which makes locally normalized, independent\n",
    "# tag classifications at each time step\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tag_vocab_size\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True, batch_first=True).to(device)\n",
    "        self.tag_projection_layer = nn.Linear(hidden_dim, self.tagset_size).to(device)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    def compute_lstm_emission_features(self, sentence):\n",
    "        hidden = self.init_hidden()\n",
    "        embeds = self.dropout(self.word_embeds(sentence))\n",
    "        bilstm_out, hidden = self.bilstm(embeds, hidden)\n",
    "        bilstm_out = self.dropout(bilstm_out)\n",
    "        bilstm_out = bilstm_out\n",
    "        bilstm_feats = self.tag_projection_layer(bilstm_out)\n",
    "        return bilstm_feats\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
    "\n",
    "        return bilstm_feats.max(-1)[0].sum(), bilstm_feats.argmax(-1)\n",
    "\n",
    "    def loss(self, sentence, tags):\n",
    "        bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
    "        # transform predictions to (n_examples, n_classes) and ground truth to (n_examples)\n",
    "        return torch.nn.functional.cross_entropy(\n",
    "              bilstm_feats.view(-1, self.tagset_size), \n",
    "              tags.view(-1), \n",
    "              reduction='sum'\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH7JGSDAruUg"
   },
   "source": [
    "## Train / Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gw2He2cgrrF1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_data, valid_data, word_vocab, label_vocab, epochs, log_interval=25):\n",
    "    losses_per_epoch = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f'--- EPOCH {epoch} ---')\n",
    "        model.train()\n",
    "        losses_per_epoch.append([])\n",
    "        for i, (sent, tags) in enumerate(train_data):\n",
    "            model.zero_grad()\n",
    "            sent, tags = sent.to(device), tags.to(device)\n",
    "            sent = sent.unsqueeze(0)\n",
    "            tags = tags.unsqueeze(0)\n",
    "            loss = model.loss(sent, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_per_epoch[-1].append(loss.detach().cpu().item())\n",
    "            if i > 0 and i % log_interval == 0:\n",
    "                print(f'Avg loss over last {log_interval} updates: {np.mean(losses_per_epoch[-1][-log_interval:])}')\n",
    "\n",
    "        evaluate(model, valid_data, word_vocab, label_vocab)\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, word_vocab, label_vocab):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    scores = []\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    sents = []\n",
    "    for i, (sent, tags) in enumerate(dataset):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        with torch.no_grad():\n",
    "            sent, tags = sent.to(device), tags.to(device)\n",
    "            sent = sent.unsqueeze(0)\n",
    "            tags = tags.unsqueeze(0)\n",
    "            losses.append(model.loss(sent, tags).cpu().detach().item())\n",
    "            score, pred_tag_seq = model(sent)\n",
    "            scores.append(score)\n",
    "            try:\n",
    "                true_tags.append([label_vocab.itos[i] for i in tags.tolist()[0]])\n",
    "                pred_tags.append([ label_vocab.itos[0] if i == 9 else label_vocab.itos[i] for i in pred_tag_seq[0]])\n",
    "            except:\n",
    "                print(pred_tag_seq)\n",
    "                pass\n",
    "\n",
    "            sents.append([word_vocab.itos[i] for i in sent[0]])\n",
    "    \n",
    "    print('Avg evaluation loss:', np.mean(losses))\n",
    "    a = [tag for tags in true_tags for tag in tags]\n",
    "    b = [tag for tags in pred_tags for tag in tags]\n",
    "    print(conlleval.evaluate(a, b, verbose=True))\n",
    "\n",
    "    scores_token = precision_recall_fscore_support(a,b, average=None, labels=label_vocab.itos)\n",
    "    for i in range(len(label_vocab.itos)):\n",
    "        print(label_vocab.itos[i] , \"precision\", scores_token[0][i], \"recall\", scores_token[1][i], \"f1\", scores_token[2][i], \"count\", scores_token[3][i])\n",
    "#     print('\\n5 random evaluation samples:')\n",
    "#     for i in np.random.randint(0, len(sents), size=2):\n",
    "#         print('SENT:', ' '.join(sents[i]))\n",
    "#         print('TRUE:', ' '.join(true_tags[i]))\n",
    "#         print('PRED:', ' '.join(pred_tags[i]))\n",
    "    return sents, true_tags, pred_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdJsc_y6rxdC"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nVyfoJfZry4-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 0 ---\n",
      "Avg loss over last 500 updates: 9.35207405039668\n",
      "Avg loss over last 500 updates: 8.168168832413853\n",
      "Avg loss over last 500 updates: 6.62803889903985\n",
      "Avg loss over last 500 updates: 5.7294120835699145\n",
      "Avg loss over last 500 updates: 5.023957918460248\n",
      "Avg loss over last 500 updates: 5.084533827416599\n",
      "Avg evaluation loss: 4.702194057154993\n",
      "processed 11170 tokens with 1231 phrases; found: 743 phrases; correct: 471.\n",
      "accuracy:  44.05%; (non-O)\n",
      "accuracy:  89.64%; precision:  63.39%; recall:  38.26%; FB1:  47.72\n",
      "              LOC: precision:  80.43%; recall:  40.77%; FB1:  54.11  184\n",
      "             MISC: precision:  53.23%; recall:  17.19%; FB1:  25.98  62\n",
      "              ORG: precision:  60.31%; recall:  38.11%; FB1:  46.71  194\n",
      "              PER: precision:  57.10%; recall:  46.88%; FB1:  51.49  303\n",
      "(63.39165545087483, 38.26157595450853, 47.72036474164134)\n",
      "O precision 0.9116366514941618 recall 0.9849262347658756 f1 0.9468653648509764 count 9354\n",
      "I-PER precision 0.7270992366412213 recall 0.5971786833855799 f1 0.6557659208261617 count 638\n",
      "I-ORG precision 0.7126436781609196 recall 0.3795918367346939 f1 0.4953395472703064 count 490\n",
      "I-LOC precision 0.8564593301435407 recall 0.4261904761904762 f1 0.5691573926868044 count 420\n",
      "I-MISC precision 0.7714285714285715 recall 0.20532319391634982 f1 0.32432432432432434 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss over last 500 updates: 4.534435162678128\n",
      "Avg loss over last 500 updates: 4.958389579779817\n",
      "Avg loss over last 500 updates: 4.050141175725388\n",
      "Avg loss over last 500 updates: 3.8272592019468576\n",
      "Avg loss over last 500 updates: 3.3551137455590068\n",
      "Avg loss over last 500 updates: 3.4140576019254003\n",
      "Avg evaluation loss: 3.7935850188352833\n",
      "processed 11170 tokens with 1231 phrases; found: 954 phrases; correct: 635.\n",
      "accuracy:  57.98%; (non-O)\n",
      "accuracy:  91.76%; precision:  66.56%; recall:  51.58%; FB1:  58.12\n",
      "              LOC: precision:  85.19%; recall:  57.02%; FB1:  68.32  243\n",
      "             MISC: precision:  72.90%; recall:  40.62%; FB1:  52.17  107\n",
      "              ORG: precision:  51.85%; recall:  45.60%; FB1:  48.53  270\n",
      "              PER: precision:  62.87%; recall:  56.91%; FB1:  59.74  334\n",
      "(66.56184486373165, 51.58407798537774, 58.12356979405035)\n",
      "O precision 0.9376083188908145 recall 0.9832157365832799 f1 0.9598705839377968 count 9354\n",
      "I-PER precision 0.784965034965035 recall 0.7037617554858934 f1 0.7421487603305784 count 638\n",
      "I-ORG precision 0.6456692913385826 recall 0.5020408163265306 f1 0.5648679678530425 count 490\n",
      "I-LOC precision 0.8861209964412812 recall 0.5928571428571429 f1 0.7104136947218259 count 420\n",
      "I-MISC precision 0.8582677165354331 recall 0.4144486692015209 f1 0.558974358974359 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 2 ---\n",
      "Avg loss over last 500 updates: 3.1436448654587403\n",
      "Avg loss over last 500 updates: 3.553026790623775\n",
      "Avg loss over last 500 updates: 2.9895624154577947\n",
      "Avg loss over last 500 updates: 2.858613094979206\n",
      "Avg loss over last 500 updates: 2.499575228369\n",
      "Avg loss over last 500 updates: 2.667385753120609\n",
      "Avg evaluation loss: 3.33570193919389\n",
      "processed 11170 tokens with 1231 phrases; found: 1008 phrases; correct: 708.\n",
      "accuracy:  63.71%; (non-O)\n",
      "accuracy:  92.91%; precision:  70.24%; recall:  57.51%; FB1:  63.24\n",
      "              LOC: precision:  84.64%; recall:  65.29%; FB1:  73.72  280\n",
      "             MISC: precision:  77.87%; recall:  49.48%; FB1:  60.51  122\n",
      "              ORG: precision:  60.00%; recall:  46.91%; FB1:  52.65  240\n",
      "              PER: precision:  63.39%; recall:  62.87%; FB1:  63.13  366\n",
      "(70.23809523809523, 57.51421608448416, 63.24251898168824)\n",
      "O precision 0.947200821777093 recall 0.9857814838571735 f1 0.9661061344229662 count 9354\n",
      "I-PER precision 0.7767145135566188 recall 0.7633228840125392 f1 0.7699604743083005 count 638\n",
      "I-ORG precision 0.7757575757575758 recall 0.5224489795918368 f1 0.624390243902439 count 490\n",
      "I-LOC precision 0.8636363636363636 recall 0.6785714285714286 f1 0.76 count 420\n",
      "I-MISC precision 0.8716216216216216 recall 0.49049429657794674 f1 0.6277372262773723 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 3 ---\n",
      "Avg loss over last 500 updates: 2.496089549539963\n",
      "Avg loss over last 500 updates: 2.84248732824712\n",
      "Avg loss over last 500 updates: 2.3940867504754313\n",
      "Avg loss over last 500 updates: 2.278484948532545\n",
      "Avg loss over last 500 updates: 2.020020150836894\n",
      "Avg loss over last 500 updates: 2.3608079937584487\n",
      "Avg evaluation loss: 3.1477844345472477\n",
      "processed 11170 tokens with 1231 phrases; found: 1044 phrases; correct: 769.\n",
      "accuracy:  67.57%; (non-O)\n",
      "accuracy:  93.55%; precision:  73.66%; recall:  62.47%; FB1:  67.60\n",
      "              LOC: precision:  86.99%; recall:  69.97%; FB1:  77.56  292\n",
      "             MISC: precision:  79.07%; recall:  53.12%; FB1:  63.55  129\n",
      "              ORG: precision:  60.22%; recall:  54.72%; FB1:  57.34  279\n",
      "              PER: precision:  71.22%; recall:  66.40%; FB1:  68.72  344\n",
      "(73.65900383141762, 62.469536961819664, 67.6043956043956)\n",
      "O precision 0.9522973670624677 recall 0.9859952961299978 f1 0.9688534061662902 count 9354\n",
      "I-PER precision 0.8352745424292846 recall 0.786833855799373 f1 0.8103309120258274 count 638\n",
      "I-ORG precision 0.7403598971722365 recall 0.5877551020408164 f1 0.6552901023890785 count 490\n",
      "I-LOC precision 0.8820058997050148 recall 0.7119047619047619 f1 0.787878787878788 count 420\n",
      "I-MISC precision 0.8846153846153846 recall 0.5247148288973384 f1 0.6587112171837709 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 4 ---\n",
      "Avg loss over last 500 updates: 1.9805482647739614\n",
      "Avg loss over last 500 updates: 2.2362417751084127\n",
      "Avg loss over last 500 updates: 1.9698085790203044\n",
      "Avg loss over last 500 updates: 1.9185324108087816\n",
      "Avg loss over last 500 updates: 1.7309984774869582\n",
      "Avg loss over last 500 updates: 1.8627708921152748\n",
      "Avg evaluation loss: 3.148096726381719\n",
      "processed 11170 tokens with 1231 phrases; found: 1073 phrases; correct: 784.\n",
      "accuracy:  69.00%; (non-O)\n",
      "accuracy:  93.72%; precision:  73.07%; recall:  63.69%; FB1:  68.06\n",
      "              LOC: precision:  87.17%; recall:  73.00%; FB1:  79.46  304\n",
      "             MISC: precision:  77.78%; recall:  54.69%; FB1:  64.22  135\n",
      "              ORG: precision:  59.17%; recall:  55.70%; FB1:  57.38  289\n",
      "              PER: precision:  70.43%; recall:  65.85%; FB1:  68.07  345\n",
      "(73.06616961789375, 63.6880584890333, 68.05555555555554)\n",
      "O precision 0.955120232172471 recall 0.9851400470387001 f1 0.9698979054836333 count 9354\n",
      "I-PER precision 0.8394648829431438 recall 0.786833855799373 f1 0.8122977346278317 count 638\n",
      "I-ORG precision 0.7356608478802993 recall 0.6020408163265306 f1 0.6621773288439954 count 490\n",
      "I-LOC precision 0.8798882681564246 recall 0.75 f1 0.8097686375321338 count 420\n",
      "I-MISC precision 0.8545454545454545 recall 0.5361216730038023 f1 0.6588785046728972 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 5 ---\n",
      "Avg loss over last 500 updates: 1.8023981835642657\n",
      "Avg loss over last 500 updates: 1.9307724068871759\n",
      "Avg loss over last 500 updates: 1.5219206798910148\n",
      "Avg loss over last 500 updates: 1.6835349312543864\n",
      "Avg loss over last 500 updates: 1.4608793862914822\n",
      "Avg loss over last 500 updates: 1.737021259012602\n",
      "Avg evaluation loss: 3.049683846443391\n",
      "processed 11170 tokens with 1231 phrases; found: 1122 phrases; correct: 808.\n",
      "accuracy:  70.98%; (non-O)\n",
      "accuracy:  93.82%; precision:  72.01%; recall:  65.64%; FB1:  68.68\n",
      "              LOC: precision:  86.62%; recall:  74.93%; FB1:  80.35  314\n",
      "             MISC: precision:  78.36%; recall:  54.69%; FB1:  64.42  134\n",
      "              ORG: precision:  63.31%; recall:  57.33%; FB1:  60.17  278\n",
      "              PER: precision:  64.39%; recall:  69.11%; FB1:  66.67  396\n",
      "(72.01426024955437, 65.63769293257515, 68.67828304292392)\n",
      "O precision 0.9590942293644996 recall 0.9825742997648065 f1 0.9706922955061519 count 9354\n",
      "I-PER precision 0.7843137254901961 recall 0.8150470219435737 f1 0.7993850883935434 count 638\n",
      "I-ORG precision 0.7669172932330827 recall 0.6244897959183674 f1 0.688413948256468 count 490\n",
      "I-LOC precision 0.8910614525139665 recall 0.7595238095238095 f1 0.8200514138817482 count 420\n",
      "I-MISC precision 0.8622754491017964 recall 0.5475285171102662 f1 0.6697674418604652 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 6 ---\n",
      "Avg loss over last 500 updates: 1.5470901150688838\n",
      "Avg loss over last 500 updates: 1.7279454644473484\n",
      "Avg loss over last 500 updates: 1.5664065654827217\n",
      "Avg loss over last 500 updates: 1.482665187684217\n",
      "Avg loss over last 500 updates: 1.3970821812016663\n",
      "Avg loss over last 500 updates: 1.5405383788212101\n",
      "Avg evaluation loss: 3.119192914674967\n",
      "processed 11170 tokens with 1231 phrases; found: 1119 phrases; correct: 822.\n",
      "accuracy:  71.37%; (non-O)\n",
      "accuracy:  93.96%; precision:  73.46%; recall:  66.77%; FB1:  69.96\n",
      "              LOC: precision:  87.86%; recall:  75.76%; FB1:  81.36  313\n",
      "             MISC: precision:  77.46%; recall:  57.29%; FB1:  65.87  142\n",
      "              ORG: precision:  62.15%; recall:  58.31%; FB1:  60.17  288\n",
      "              PER: precision:  68.62%; recall:  69.92%; FB1:  69.26  376\n",
      "(73.45844504021449, 66.77497969130788, 69.95744680851064)\n",
      "O precision 0.9607310704960835 recall 0.9834295488561043 f1 0.9719478049553595 count 9354\n",
      "I-PER precision 0.8056426332288401 recall 0.8056426332288401 f1 0.8056426332288401 count 638\n",
      "I-ORG precision 0.7469879518072289 recall 0.6326530612244898 f1 0.6850828729281768 count 490\n",
      "I-LOC precision 0.8991596638655462 recall 0.7642857142857142 f1 0.8262548262548263 count 420\n",
      "I-MISC precision 0.8162162162162162 recall 0.5741444866920152 f1 0.6741071428571428 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 7 ---\n",
      "Avg loss over last 500 updates: 1.4060663823442368\n",
      "Avg loss over last 500 updates: 1.4835989599070074\n",
      "Avg loss over last 500 updates: 1.4635226177821765\n",
      "Avg loss over last 500 updates: 1.381074431893492\n",
      "Avg loss over last 500 updates: 1.1979586405511478\n",
      "Avg loss over last 500 updates: 1.4223460803746826\n",
      "Avg evaluation loss: 3.2101241473532345\n",
      "processed 11170 tokens with 1231 phrases; found: 1125 phrases; correct: 830.\n",
      "accuracy:  71.92%; (non-O)\n",
      "accuracy:  94.02%; precision:  73.78%; recall:  67.42%; FB1:  70.46\n",
      "              LOC: precision:  84.98%; recall:  77.96%; FB1:  81.32  333\n",
      "             MISC: precision:  76.60%; recall:  56.25%; FB1:  64.86  141\n",
      "              ORG: precision:  64.18%; recall:  58.96%; FB1:  61.46  282\n",
      "              PER: precision:  69.92%; recall:  69.92%; FB1:  69.92  369\n",
      "(73.77777777777777, 67.42485783915517, 70.45840407470288)\n",
      "O precision 0.9593156686834967 recall 0.9831088304468677 f1 0.971066525871172 count 9354\n",
      "I-PER precision 0.8322475570032574 recall 0.8009404388714734 f1 0.8162939297124601 count 638\n",
      "I-ORG precision 0.769041769041769 recall 0.6387755102040816 f1 0.6978818283166109 count 490\n",
      "I-LOC precision 0.8694516971279374 recall 0.7928571428571428 f1 0.8293897882938979 count 420\n",
      "I-MISC precision 0.8324022346368715 recall 0.5665399239543726 f1 0.67420814479638 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 8 ---\n",
      "Avg loss over last 500 updates: 1.2316292872095642\n",
      "Avg loss over last 500 updates: 1.4295319541266533\n",
      "Avg loss over last 500 updates: 1.1914200159884731\n",
      "Avg loss over last 500 updates: 1.2151559701549928\n",
      "Avg loss over last 500 updates: 1.0435020607840417\n",
      "Avg loss over last 500 updates: 1.203471519554475\n",
      "Avg evaluation loss: 3.458814347920895\n",
      "processed 11170 tokens with 1231 phrases; found: 1111 phrases; correct: 824.\n",
      "accuracy:  71.86%; (non-O)\n",
      "accuracy:  94.18%; precision:  74.17%; recall:  66.94%; FB1:  70.37\n",
      "              LOC: precision:  88.09%; recall:  77.41%; FB1:  82.40  319\n",
      "             MISC: precision:  74.83%; recall:  57.29%; FB1:  64.90  147\n",
      "              ORG: precision:  64.39%; recall:  55.37%; FB1:  59.54  264\n",
      "              PER: precision:  69.03%; recall:  71.27%; FB1:  70.13  381\n",
      "(74.16741674167416, 66.9374492282697, 70.36720751494448)\n",
      "O precision 0.9598958333333333 recall 0.9851400470387001 f1 0.9723541205022687 count 9354\n",
      "I-PER precision 0.8167701863354038 recall 0.8244514106583072 f1 0.8205928237129485 count 638\n",
      "I-ORG precision 0.7978723404255319 recall 0.6122448979591837 f1 0.6928406466512702 count 490\n",
      "I-LOC precision 0.8986301369863013 recall 0.780952380952381 f1 0.8356687898089171 count 420\n",
      "I-MISC precision 0.8162162162162162 recall 0.5741444866920152 f1 0.6741071428571428 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 9 ---\n",
      "Avg loss over last 500 updates: 1.0936059125644688\n",
      "Avg loss over last 500 updates: 1.2782912199084748\n",
      "Avg loss over last 500 updates: 1.1035430477672035\n",
      "Avg loss over last 500 updates: 1.1665655608095702\n",
      "Avg loss over last 500 updates: 1.015911700609781\n",
      "Avg loss over last 500 updates: 1.1910294114936095\n",
      "Avg evaluation loss: 3.444161086631476\n",
      "processed 11170 tokens with 1231 phrases; found: 1111 phrases; correct: 834.\n",
      "accuracy:  72.03%; (non-O)\n",
      "accuracy:  94.23%; precision:  75.07%; recall:  67.75%; FB1:  71.22\n",
      "              LOC: precision:  86.48%; recall:  75.76%; FB1:  80.76  318\n",
      "             MISC: precision:  78.72%; recall:  57.81%; FB1:  66.67  141\n",
      "              ORG: precision:  67.16%; recall:  58.63%; FB1:  62.61  268\n",
      "              PER: precision:  69.79%; recall:  72.63%; FB1:  71.18  384\n",
      "(75.06750675067508, 67.7497969130788, 71.2211784799317)\n",
      "O precision 0.9596043727225403 recall 0.9853538593115245 f1 0.9723086660688854 count 9354\n",
      "I-PER precision 0.8207109737248841 recall 0.8322884012539185 f1 0.8264591439688717 count 638\n",
      "I-ORG precision 0.8101604278074866 recall 0.6183673469387755 f1 0.7013888888888888 count 490\n",
      "I-LOC precision 0.8828337874659401 recall 0.7714285714285715 f1 0.8233799237611182 count 420\n",
      "I-MISC precision 0.847457627118644 recall 0.5703422053231939 f1 0.6818181818181818 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n"
     ]
    }
   ],
   "source": [
    "# Train BiLSTM Tagger Baseline\n",
    "model = BiLSTMTagger(len(word_vocab), len(label_vocab), 128, 256).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model, train_data, valid_data, word_vocab, label_vocab, epochs=10, log_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freqs': Counter({'I-ORG': 2258,\n",
       "          'O': 38899,\n",
       "          'I-PER': 2544,\n",
       "          'I-LOC': 1836,\n",
       "          'I-MISC': 1011,\n",
       "          'B-MISC': 9,\n",
       "          'B-ORG': 5,\n",
       "          'B-LOC': 3}),\n",
       " 'itos': ['O',\n",
       "  'I-PER',\n",
       "  'I-ORG',\n",
       "  'I-LOC',\n",
       "  'I-MISC',\n",
       "  'B-MISC',\n",
       "  'B-ORG',\n",
       "  'B-LOC'],\n",
       " 'unk_index': None,\n",
       " 'stoi': defaultdict(None,\n",
       "             {'O': 0,\n",
       "              'I-PER': 1,\n",
       "              'I-ORG': 2,\n",
       "              'I-LOC': 3,\n",
       "              'I-MISC': 4,\n",
       "              'B-MISC': 5,\n",
       "              'B-ORG': 6,\n",
       "              'B-LOC': 7}),\n",
       " 'vectors': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vocab.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg evaluation loss: 3.480769702887892\n",
      "processed 11170 tokens with 1231 phrases; found: 1117 phrases; correct: 844.\n",
      "accuracy:  72.69%; (non-O)\n",
      "accuracy:  94.32%; precision:  75.56%; recall:  68.56%; FB1:  71.89\n",
      "              LOC: precision:  87.50%; recall:  77.13%; FB1:  81.99  320\n",
      "             MISC: precision:  79.58%; recall:  58.85%; FB1:  67.66  142\n",
      "              ORG: precision:  68.05%; recall:  58.96%; FB1:  63.18  266\n",
      "              PER: precision:  69.41%; recall:  73.17%; FB1:  71.24  389\n",
      "(75.55953446732319, 68.5621445978879, 71.89097103918229)\n",
      "O precision 0.9605962681121651 recall 0.9851400470387001 f1 0.9727133583153007 count 9354\n",
      "I-PER precision 0.8143074581430746 recall 0.8385579937304075 f1 0.8262548262548264 count 638\n",
      "I-ORG precision 0.8198924731182796 recall 0.6224489795918368 f1 0.7076566125290024 count 490\n",
      "I-LOC precision 0.8888888888888888 recall 0.780952380952381 f1 0.8314321926489227 count 420\n",
      "I-MISC precision 0.8539325842696629 recall 0.5779467680608364 f1 0.6893424036281178 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n"
     ]
    }
   ],
   "source": [
    "sents, true_tags, pred_tags = evaluate(model, valid_data, word_vocab, label_vocab )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "for i,sent in enumerate(sents):\n",
    "    for j,word in enumerate(sent):\n",
    "        if word == \"<unk>\":\n",
    "            a.append(true_tags[i][j])\n",
    "            b.append(pred_tags[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O precision 81.05 recall 90.85 f1 85.67 count 1431\n",
      "I-PER precision 76.38 recall 79.96 f1 78.13 count 449\n",
      "I-ORG precision 69.83 recall 51.44 f1 59.24 count 243\n",
      "I-LOC precision 75.51 recall 36.63 f1 49.33 count 101\n",
      "I-MISC precision 52.63 recall 10.87 f1 18.02 count 92\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n"
     ]
    }
   ],
   "source": [
    "scores_token = precision_recall_fscore_support(a,b, average=None, labels=label_vocab.itos)\n",
    "\n",
    "for i in range(len(label_vocab.itos)):\n",
    "    print(label_vocab.itos[i] , \"precision\", round(scores_token[0][i] * 100,2), \n",
    "          \"recall\", round(scores_token[1][i] * 100, 2), \"f1\", round(scores_token[2][i] * 100, 2), \"count\", scores_token[3][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_token = precision_recall_fscore_support(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cse291_assignment2_starter_code.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
