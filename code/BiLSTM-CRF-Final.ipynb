{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHnbGR5wpCL8"
   },
   "source": [
    "# CSE 291 Assignment 2 BiLSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs2O4920pCob"
   },
   "source": [
    "## Download Data/Eval Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hmfarI0hpHj6"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
    "# !wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/train.data.quad\n",
    "# !wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/dev.data.quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0CMvXrmwpNCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import conlleval\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pdb \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "torch.manual_seed(291)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOBmqHytpTGs"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GKfmSZs8pPBV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train word vocab: 3947 symbols.\n",
      "Train label vocab: 8 symbols: ['O', 'I-PER', 'I-ORG', 'I-LOC', 'I-MISC', 'B-MISC', 'B-ORG', 'B-LOC']\n",
      "Val word vocab: 1078 symbols.\n",
      "Train data: 3420 sentences.\n",
      "Valid data: 800\n",
      "Pusan 0000 0000 0000 0000 0000 0000\n",
      "I-ORG O O O O O O\n",
      "Earlier this month , <unk> denied a Kabul government statement that the two sides had agreed to a ceasefire in the north .\n",
      "O O O O I-PER O O I-LOC O O O O O O O O O O O O O O O\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = 'train.data.quad'\n",
    "VALID_DATA = 'dev.data.quad'\n",
    "UNK = '<unk>'\n",
    "PAD = '<pad>'\n",
    "START_TAG = \"<start>\"  # you can add this explicitly or use it implicitly in your CRF layer\n",
    "STOP_TAG = \"<stop>\"    # you can add this explicitly or use it implicitly in your CRF layer\n",
    "\n",
    "\n",
    "def read_conll_sentence(path):\n",
    "    \"\"\" Read a CONLL-format sentence into vocab objects\n",
    "    Args:\n",
    "        :param path: path to CONLL-format data file\n",
    "        :param word_vocab: Vocabulary object for source\n",
    "        :param label_vocab: Vocabulary object for target\n",
    "    \"\"\"\n",
    "    sent = [[], []]\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line:\n",
    "                # replace numbers with 0000\n",
    "                word = line[0]\n",
    "                word = '0000' if word.isnumeric() else word\n",
    "                sent[0].append(word)\n",
    "                sent[1].append(line[3])\n",
    "            else:\n",
    "                yield sent[0], sent[1]\n",
    "                sent = [[], []]\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, word_vocab, label_vocab):\n",
    "    dataset = [\n",
    "      [\n",
    "        torch.tensor([word_vocab.stoi[word] for word in sent[0]], dtype=torch.long),\n",
    "        torch.tensor([label_vocab.stoi[label] for label in sent[1]], dtype=torch.long),\n",
    "      ]\n",
    "      for sent in dataset\n",
    "    ]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# load a list of sentences, where each word in the list is a tuple containing the word and the label\n",
    "train_data = list(read_conll_sentence(TRAIN_DATA))\n",
    "train_word_counter = Counter([word for sent in train_data for word in sent[0]])\n",
    "train_label_counter = Counter([label for sent in train_data for label in sent[1]])\n",
    "word_vocab = Vocab(train_word_counter, specials=(UNK, PAD), min_freq=2)\n",
    "\n",
    "label_vocab = Vocab(train_label_counter, specials=(), min_freq=1)\n",
    "train_data = prepare_dataset(train_data, word_vocab, label_vocab)\n",
    "print('Train word vocab:', len(word_vocab), 'symbols.')\n",
    "print('Train label vocab:', len(label_vocab), f'symbols: {list(label_vocab.stoi.keys())}')\n",
    "\n",
    "valid_data = list(read_conll_sentence(VALID_DATA))\n",
    "\n",
    "val_word_counter = Counter([word for sent in valid_data for word in sent[0]])\n",
    "val_word_vocab = Vocab(val_word_counter, specials=(UNK, PAD), min_freq=2)\n",
    "\n",
    "print('Val word vocab:', len(val_word_vocab), 'symbols.')\n",
    "\n",
    "\n",
    "valid_data = prepare_dataset(valid_data, word_vocab, label_vocab)\n",
    "print('Train data:', len(train_data), 'sentences.')\n",
    "print('Valid data:', len(valid_data))\n",
    "\n",
    "print(' '.join([word_vocab.itos[i.item()] for i in train_data[0][0]]))\n",
    "print(' '.join([label_vocab.itos[i.item()] for i in train_data[0][1]]))\n",
    "\n",
    "print(' '.join([word_vocab.itos[i.item()] for i in valid_data[1][0]]))\n",
    "print(' '.join([label_vocab.itos[i.item()] for i in valid_data[1][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inVocabWord = set((val_word_vocab.freqs)).intersection(set(word_vocab.freqs))\n",
    "oovWord = set((val_word_vocab.freqs)).difference(set(word_vocab.freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocab.itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec.max()\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNNmZx_Uqy7q"
   },
   "source": [
    "## BiLSTMTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF():\n",
    "    def __init__(self, vocab_size, tag_vocab_size):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tag_vocab_size\n",
    "\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size)).to(device)\n",
    "\n",
    "        self.start_index = tag_vocab_size - 2\n",
    "        self.end_index = tag_vocab_size - 1\n",
    "        \n",
    "        self.transitions.data[self.start_index, :] = -10000\n",
    "        self.transitions.data[:, self.end_index] = -10000\n",
    "        \n",
    "    \n",
    "    def _viterbi_decode(self, feats):\n",
    "        backtrace = []\n",
    "        alpha = torch.full((1, self.tagset_size), -10000.0, device=device)\n",
    "        alpha[0][self.start_index] = 0\n",
    "        feats = feats.squeeze(0)\n",
    "#         pdb.set_trace()\n",
    "        for feat in feats:\n",
    "            t2 = (self.transitions.T + (feat)).T + alpha\n",
    "            t3 = t2.max(dim=1).values\n",
    "            alpha = t3 + (t2.T - t3).T.exp().sum(dim=1).log().view(1,-1)\n",
    "            backtrace.append(t2.argmax(dim=1))\n",
    "\n",
    "        vec = alpha.T + self.transitions[:, [self.end_index]]\n",
    "\n",
    "        \n",
    "        best_tag = vec.squeeze(1).argmax().item()\n",
    "\n",
    "        optimal_path = [best_tag]\n",
    "        \n",
    "        seq = reversed(backtrace[1:])\n",
    "        \n",
    "        for bp in seq: \n",
    "            best_tag = bp[best_tag].item()\n",
    "            optimal_path.append(best_tag)\n",
    "        \n",
    "        score = (vec - vec.max()).exp().sum(axis=0, keepdim=True).log() + vec.max()\n",
    "        return score,[ optimal_path[::-1]] \n",
    "    \n",
    "     \n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "\n",
    "        init_alphas[0][self.start_index] = 0.\n",
    "\n",
    "\n",
    "        forward_var = init_alphas.to(device)\n",
    "\n",
    "        feats = feats.squeeze(0)\n",
    "\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            t2 = (self.transitions.T + (feat)).T + forward_var\n",
    "            t3 = t2.max(dim=1).values\n",
    "            forward_var = t3 + (t2.T - t3).T.exp().sum(dim=1).log().view(1,-1)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.end_index]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "    \n",
    "    def _score_sentence(self, feats, tags):\n",
    "\n",
    "        score = torch.zeros(1).to(device)\n",
    "        feats = feats.squeeze(0)\n",
    "        tags = tags.squeeze(0)\n",
    "        tags = torch.cat([torch.tensor([self.start_index], dtype=torch.long).to(device), tags]).to(device)\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "\n",
    "        score = score + self.transitions[self.end_index, tags[-1]]\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "a5nVIM_Eq1ZU"
   },
   "outputs": [],
   "source": [
    "# Starter code implementing a BiLSTM Tagger\n",
    "# which makes locally normalized, independent\n",
    "# tag classifications at each time step\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tag_vocab_size\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True, batch_first=True).to(device)\n",
    "        self.tag_projection_layer = nn.Linear(hidden_dim, self.tagset_size).to(device)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.crf = CRF(vocab_size, tag_vocab_size)\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    def compute_lstm_emission_features(self, sentence):\n",
    "        hidden = self.init_hidden()\n",
    "        embeds = self.dropout(self.word_embeds(sentence))\n",
    "        bilstm_out, hidden = self.bilstm(embeds, hidden)\n",
    "        bilstm_out = self.dropout(bilstm_out)\n",
    "        bilstm_out = bilstm_out\n",
    "        bilstm_feats = self.tag_projection_layer(bilstm_out)\n",
    "        return bilstm_feats\n",
    "\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
    "\n",
    "        return self.crf._viterbi_decode(bilstm_feats)\n",
    "   \n",
    "\n",
    "    def loss(self, sentence, tags):\n",
    "        feats = self.compute_lstm_emission_features(sentence)\n",
    "        forward_score = self.crf._forward_alg(feats)\n",
    "        gold_score = self.crf._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH7JGSDAruUg"
   },
   "source": [
    "## Train / Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Gw2He2cgrrF1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_data, valid_data, word_vocab, label_vocab, epochs, log_interval=25):\n",
    "    losses_per_epoch = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f'--- EPOCH {epoch} ---')\n",
    "        model.train()\n",
    "        losses_per_epoch.append([])\n",
    "        for i, (sent, tags) in enumerate(train_data):\n",
    "            model.zero_grad()\n",
    "            sent, tags = sent.to(device), tags.to(device)\n",
    "            sent = sent.unsqueeze(0)\n",
    "            tags = tags.unsqueeze(0)\n",
    "            loss = model.loss(sent, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_per_epoch[-1].append(loss.detach().cpu().item())\n",
    "            if i > 0 and i % log_interval == 0:\n",
    "                print(f'Avg loss over last {log_interval} updates: {np.mean(losses_per_epoch[-1][-log_interval:])}')\n",
    "\n",
    "        evaluate(model, valid_data, word_vocab, label_vocab)\n",
    "\n",
    "\n",
    "def evaluate(model, dataset, word_vocab, label_vocab):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    scores = []\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    sents = []\n",
    "    for i, (sent, tags) in enumerate(dataset):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        with torch.no_grad():\n",
    "            sent, tags = sent.to(device), tags.to(device)\n",
    "            sent = sent.unsqueeze(0)\n",
    "            tags = tags.unsqueeze(0)\n",
    "            losses.append(model.loss(sent, tags).cpu().detach().item())\n",
    "            score, pred_tag_seq = model(sent)\n",
    "            scores.append(score)\n",
    "            try:\n",
    "                true_tags.append([label_vocab.itos[i] for i in tags.tolist()[0]])\n",
    "                pred_tags.append([ label_vocab.itos[0] if i == 9 else label_vocab.itos[i] for i in pred_tag_seq[0]])\n",
    "            except:\n",
    "                print(pred_tag_seq)\n",
    "                pass\n",
    "\n",
    "            sents.append([word_vocab.itos[i] for i in sent[0]])\n",
    "    \n",
    "    print('Avg evaluation loss:', np.mean(losses))\n",
    "    a = [tag for tags in true_tags for tag in tags]\n",
    "    b = [tag for tags in pred_tags for tag in tags]\n",
    "    print(conlleval.evaluate(a, b, verbose=True))\n",
    "\n",
    "    scores_token = precision_recall_fscore_support(a,b, average=None, labels=label_vocab.itos)\n",
    "        \n",
    "    for i in range(len(label_vocab.itos)):\n",
    "        print(label_vocab.itos[i] , \"precision\", scores_token[0][i], \"recall\", scores_token[1][i], \"f1\", scores_token[2][i], \"count\", scores_token[3][i])\n",
    "#     print('\\n5 random evaluation samples:')\n",
    "#     for i in np.random.randint(0, len(sents), size=2):\n",
    "#         print('SENT:', ' '.join(sents[i]))\n",
    "#         print('TRUE:', ' '.join(true_tags[i]))\n",
    "#         print('PRED:', ' '.join(pred_tags[i]))\n",
    "    return sents, true_tags, pred_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdJsc_y6rxdC"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nVyfoJfZry4-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EPOCH 0 ---\n",
      "Avg loss over last 500 updates: 10.90044804263115\n",
      "Avg loss over last 500 updates: 8.894040877342224\n",
      "Avg loss over last 500 updates: 7.136677108049392\n",
      "Avg loss over last 500 updates: 6.2077221388816834\n",
      "Avg loss over last 500 updates: 5.5089131655693055\n",
      "Avg loss over last 500 updates: 5.408730796813964\n",
      "Avg evaluation loss: 5.117264733016491\n",
      "processed 11170 tokens with 1231 phrases; found: 817 phrases; correct: 438.\n",
      "accuracy:  42.35%; (non-O)\n",
      "accuracy:  88.88%; precision:  53.61%; recall:  35.58%; FB1:  42.77\n",
      "              LOC: precision:  79.35%; recall:  40.22%; FB1:  53.38  184\n",
      "             MISC: precision:  49.15%; recall:  15.10%; FB1:  23.11  59\n",
      "              ORG: precision:  49.77%; recall:  34.85%; FB1:  41.00  215\n",
      "              PER: precision:  43.45%; recall:  42.28%; FB1:  42.86  359\n",
      "(53.61077111383109, 35.580828594638504, 42.7734375)\n",
      "O precision 0.9102564102564102 recall 0.9791533033996151 f1 0.9434487021013597 count 9354\n",
      "I-PER precision 0.6814814814814815 recall 0.5768025078369906 f1 0.6247877758913413 count 638\n",
      "I-ORG precision 0.6075085324232082 recall 0.363265306122449 f1 0.454661558109834 count 490\n",
      "I-LOC precision 0.8403755868544601 recall 0.4261904761904762 f1 0.5655608214849921 count 420\n",
      "I-MISC precision 0.7213114754098361 recall 0.16730038022813687 f1 0.2716049382716049 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss over last 500 updates: 4.909052533149719\n",
      "Avg loss over last 500 updates: 5.382206438064575\n",
      "Avg loss over last 500 updates: 4.474524360179901\n",
      "Avg loss over last 500 updates: 4.233311654567719\n",
      "Avg loss over last 500 updates: 3.79393673324585\n",
      "Avg loss over last 500 updates: 3.767577681541443\n",
      "Avg evaluation loss: 3.996925927102566\n",
      "processed 11170 tokens with 1231 phrases; found: 967 phrases; correct: 614.\n",
      "accuracy:  56.33%; (non-O)\n",
      "accuracy:  91.48%; precision:  63.50%; recall:  49.88%; FB1:  55.87\n",
      "              LOC: precision:  82.30%; recall:  55.10%; FB1:  66.01  243\n",
      "             MISC: precision:  66.12%; recall:  41.67%; FB1:  51.12  121\n",
      "              ORG: precision:  50.19%; recall:  42.67%; FB1:  46.13  261\n",
      "              PER: precision:  59.36%; recall:  55.01%; FB1:  57.10  342\n",
      "(63.49534643226473, 49.87814784727863, 55.868971792538666)\n",
      "O precision 0.9344512195121951 recall 0.9830019243104554 f1 0.9581119099718662 count 9354\n",
      "I-PER precision 0.7924187725631769 recall 0.6880877742946708 f1 0.7365771812080536 count 638\n",
      "I-ORG precision 0.6424581005586593 recall 0.46938775510204084 f1 0.5424528301886793 count 490\n",
      "I-LOC precision 0.8556338028169014 recall 0.5785714285714286 f1 0.6903409090909091 count 420\n",
      "I-MISC precision 0.8283582089552238 recall 0.4220532319391635 f1 0.5591939546599496 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 2 ---\n",
      "Avg loss over last 500 updates: 3.432263619184494\n",
      "Avg loss over last 500 updates: 3.9450449166297914\n",
      "Avg loss over last 500 updates: 3.190984775066376\n",
      "Avg loss over last 500 updates: 3.140933470249176\n",
      "Avg loss over last 500 updates: 2.844809525966644\n",
      "Avg loss over last 500 updates: 2.94657630109787\n",
      "Avg evaluation loss: 3.535016695857048\n",
      "processed 11170 tokens with 1231 phrases; found: 1001 phrases; correct: 689.\n",
      "accuracy:  62.11%; (non-O)\n",
      "accuracy:  92.65%; precision:  68.83%; recall:  55.97%; FB1:  61.74\n",
      "              LOC: precision:  84.27%; recall:  61.98%; FB1:  71.43  267\n",
      "             MISC: precision:  70.00%; recall:  47.40%; FB1:  56.52  130\n",
      "              ORG: precision:  57.89%; recall:  46.58%; FB1:  51.62  247\n",
      "              PER: precision:  64.43%; recall:  62.33%; FB1:  63.36  357\n",
      "(68.83116883116884, 55.97075548334687, 61.73835125448027)\n",
      "O precision 0.9427461404764339 recall 0.9857814838571735 f1 0.9637836425398485 count 9354\n",
      "I-PER precision 0.8121827411167513 recall 0.7523510971786834 f1 0.7811228641171685 count 638\n",
      "I-ORG precision 0.7470588235294118 recall 0.5183673469387755 f1 0.6120481927710842 count 490\n",
      "I-LOC precision 0.8737864077669902 recall 0.6428571428571429 f1 0.7407407407407407 count 420\n",
      "I-MISC precision 0.8322147651006712 recall 0.4714828897338403 f1 0.6019417475728156 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 3 ---\n",
      "Avg loss over last 500 updates: 2.7147335500717165\n",
      "Avg loss over last 500 updates: 2.988064967632294\n",
      "Avg loss over last 500 updates: 2.496230431318283\n",
      "Avg loss over last 500 updates: 2.4888058433532714\n",
      "Avg loss over last 500 updates: 2.2626509985923766\n",
      "Avg loss over last 500 updates: 2.5006999144554136\n",
      "Avg evaluation loss: 3.300646953582764\n",
      "processed 11170 tokens with 1231 phrases; found: 1024 phrases; correct: 722.\n",
      "accuracy:  65.03%; (non-O)\n",
      "accuracy:  93.15%; precision:  70.51%; recall:  58.65%; FB1:  64.04\n",
      "              LOC: precision:  89.66%; recall:  64.46%; FB1:  75.00  261\n",
      "             MISC: precision:  75.19%; recall:  52.08%; FB1:  61.54  133\n",
      "              ORG: precision:  59.16%; recall:  50.49%; FB1:  54.48  262\n",
      "              PER: precision:  63.32%; recall:  63.14%; FB1:  63.23  368\n",
      "(70.5078125, 58.651502843216896, 64.03547671840354)\n",
      "O precision 0.9478010686395396 recall 0.9861022022664101 f1 0.966572356701247 count 9354\n",
      "I-PER precision 0.803921568627451 recall 0.7711598746081505 f1 0.7872 count 638\n",
      "I-ORG precision 0.7513661202185792 recall 0.5612244897959183 f1 0.6425233644859812 count 490\n",
      "I-LOC precision 0.9180327868852459 recall 0.6666666666666666 f1 0.7724137931034483 count 420\n",
      "I-MISC precision 0.864516129032258 recall 0.5095057034220533 f1 0.6411483253588517 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 4 ---\n",
      "Avg loss over last 500 updates: 2.0751144628524782\n",
      "Avg loss over last 500 updates: 2.5047154364585875\n",
      "Avg loss over last 500 updates: 2.117709962844849\n",
      "Avg loss over last 500 updates: 2.0786099133491516\n",
      "Avg loss over last 500 updates: 2.0079400997161865\n",
      "Avg loss over last 500 updates: 2.0471601717472074\n",
      "Avg evaluation loss: 3.1572660318017007\n",
      "processed 11170 tokens with 1231 phrases; found: 1085 phrases; correct: 774.\n",
      "accuracy:  67.95%; (non-O)\n",
      "accuracy:  93.57%; precision:  71.34%; recall:  62.88%; FB1:  66.84\n",
      "              LOC: precision:  86.62%; recall:  71.35%; FB1:  78.25  299\n",
      "             MISC: precision:  75.18%; recall:  55.21%; FB1:  63.66  141\n",
      "              ORG: precision:  56.36%; recall:  53.42%; FB1:  54.85  291\n",
      "              PER: precision:  69.21%; recall:  66.40%; FB1:  67.77  354\n",
      "(71.33640552995392, 62.875710804224205, 66.83937823834198)\n",
      "O precision 0.9538493377483444 recall 0.9854607654479367 f1 0.9693974129771795 count 9354\n",
      "I-PER precision 0.8490566037735849 recall 0.7758620689655172 f1 0.8108108108108107 count 638\n",
      "I-ORG precision 0.7121951219512195 recall 0.5959183673469388 f1 0.6488888888888888 count 490\n",
      "I-LOC precision 0.884393063583815 recall 0.7285714285714285 f1 0.7989556135770234 count 420\n",
      "I-MISC precision 0.844311377245509 recall 0.5361216730038023 f1 0.6558139534883721 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 5 ---\n",
      "Avg loss over last 500 updates: 1.8067271420955657\n",
      "Avg loss over last 500 updates: 2.2018813247680664\n",
      "Avg loss over last 500 updates: 1.7335256905555725\n",
      "Avg loss over last 500 updates: 1.8108779001235962\n",
      "Avg loss over last 500 updates: 1.5590198187828064\n",
      "Avg loss over last 500 updates: 1.7984125804901123\n",
      "Avg evaluation loss: 3.120398830473423\n",
      "processed 11170 tokens with 1231 phrases; found: 1085 phrases; correct: 796.\n",
      "accuracy:  69.66%; (non-O)\n",
      "accuracy:  93.87%; precision:  73.36%; recall:  64.66%; FB1:  68.74\n",
      "              LOC: precision:  90.33%; recall:  74.66%; FB1:  81.75  300\n",
      "             MISC: precision:  71.81%; recall:  55.73%; FB1:  62.76  149\n",
      "              ORG: precision:  60.78%; recall:  56.03%; FB1:  58.31  283\n",
      "              PER: precision:  69.69%; recall:  66.67%; FB1:  68.14  353\n",
      "(73.36405529953916, 64.66287571080423, 68.73920552677029)\n",
      "O precision 0.9553414153973682 recall 0.9856745777207612 f1 0.9702709813207051 count 9354\n",
      "I-PER precision 0.8460236886632826 recall 0.7836990595611285 f1 0.8136696501220505 count 638\n",
      "I-ORG precision 0.7506172839506173 recall 0.6204081632653061 f1 0.6793296089385475 count 490\n",
      "I-LOC precision 0.9188405797101449 recall 0.7547619047619047 f1 0.8287581699346405 count 420\n",
      "I-MISC precision 0.8135593220338984 recall 0.5475285171102662 f1 0.6545454545454547 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 6 ---\n",
      "Avg loss over last 500 updates: 1.5527831783294679\n",
      "Avg loss over last 500 updates: 1.861496187210083\n",
      "Avg loss over last 500 updates: 1.6110513548851013\n",
      "Avg loss over last 500 updates: 1.5774495315551758\n",
      "Avg loss over last 500 updates: 1.48092330121994\n",
      "Avg loss over last 500 updates: 1.6415904631614686\n",
      "Avg evaluation loss: 3.173076501637697\n",
      "processed 11170 tokens with 1231 phrases; found: 1130 phrases; correct: 801.\n",
      "accuracy:  70.21%; (non-O)\n",
      "accuracy:  93.68%; precision:  70.88%; recall:  65.07%; FB1:  67.85\n",
      "              LOC: precision:  87.82%; recall:  75.48%; FB1:  81.19  312\n",
      "             MISC: precision:  75.17%; recall:  58.33%; FB1:  65.69  149\n",
      "              ORG: precision:  54.32%; recall:  57.33%; FB1:  55.78  324\n",
      "              PER: precision:  69.28%; recall:  64.77%; FB1:  66.95  345\n",
      "(70.88495575221239, 65.06904955320877, 67.85260482846252)\n",
      "O precision 0.9585854370957646 recall 0.982360487491982 f1 0.9703273495248153 count 9354\n",
      "I-PER precision 0.8457538994800693 recall 0.7648902821316614 f1 0.8032921810699589 count 638\n",
      "I-ORG precision 0.6794871794871795 recall 0.6489795918367347 f1 0.6638830897703549 count 490\n",
      "I-LOC precision 0.8916666666666667 recall 0.7642857142857142 f1 0.8230769230769232 count 420\n",
      "I-MISC precision 0.8314606741573034 recall 0.5627376425855514 f1 0.671201814058957 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 7 ---\n",
      "Avg loss over last 500 updates: 1.4313379492759704\n",
      "Avg loss over last 500 updates: 1.6012636966705323\n",
      "Avg loss over last 500 updates: 1.4882144846916199\n",
      "Avg loss over last 500 updates: 1.500822335243225\n",
      "Avg loss over last 500 updates: 1.339665497303009\n",
      "Avg loss over last 500 updates: 1.5119269304275513\n",
      "Avg evaluation loss: 3.294788536578417\n",
      "processed 11170 tokens with 1231 phrases; found: 1110 phrases; correct: 812.\n",
      "accuracy:  71.04%; (non-O)\n",
      "accuracy:  93.93%; precision:  73.15%; recall:  65.96%; FB1:  69.37\n",
      "              LOC: precision:  89.00%; recall:  75.76%; FB1:  81.85  309\n",
      "             MISC: precision:  77.40%; recall:  58.85%; FB1:  66.86  146\n",
      "              ORG: precision:  60.00%; recall:  55.70%; FB1:  57.77  285\n",
      "              PER: precision:  68.38%; recall:  68.56%; FB1:  68.47  370\n",
      "(73.15315315315316, 65.96263200649878, 69.37206322084579)\n",
      "O precision 0.9591411298728372 recall 0.983750267265341 f1 0.9712898458940257 count 9354\n",
      "I-PER precision 0.8309178743961353 recall 0.8087774294670846 f1 0.8196981731532963 count 638\n",
      "I-ORG precision 0.7203791469194313 recall 0.6204081632653061 f1 0.6666666666666667 count 490\n",
      "I-LOC precision 0.8997214484679665 recall 0.7690476190476191 f1 0.8292682926829267 count 420\n",
      "I-MISC precision 0.8497109826589595 recall 0.55893536121673 f1 0.6743119266055047 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 8 ---\n",
      "Avg loss over last 500 updates: 1.339733476638794\n",
      "Avg loss over last 500 updates: 1.4535602531433105\n",
      "Avg loss over last 500 updates: 1.1751929364204408\n",
      "Avg loss over last 500 updates: 1.2478724746704102\n",
      "Avg loss over last 500 updates: 1.1810068831443787\n",
      "Avg loss over last 500 updates: 1.3789316287040712\n",
      "Avg evaluation loss: 3.4035628634691237\n",
      "processed 11170 tokens with 1231 phrases; found: 1119 phrases; correct: 808.\n",
      "accuracy:  71.09%; (non-O)\n",
      "accuracy:  93.96%; precision:  72.21%; recall:  65.64%; FB1:  68.77\n",
      "              LOC: precision:  88.10%; recall:  75.48%; FB1:  81.31  311\n",
      "             MISC: precision:  76.92%; recall:  57.29%; FB1:  65.67  143\n",
      "              ORG: precision:  60.50%; recall:  55.37%; FB1:  57.82  281\n",
      "              PER: precision:  66.15%; recall:  68.83%; FB1:  67.46  384\n",
      "(72.20732797140303, 65.63769293257515, 68.7659574468085)\n",
      "O precision 0.95875 recall 0.9839640795381654 f1 0.9711934156378601 count 9354\n",
      "I-PER precision 0.8145800316957211 recall 0.8056426332288401 f1 0.8100866824271079 count 638\n",
      "I-ORG precision 0.7524752475247525 recall 0.6204081632653061 f1 0.6800894854586129 count 490\n",
      "I-LOC precision 0.9005524861878453 recall 0.7761904761904762 f1 0.833759590792839 count 420\n",
      "I-MISC precision 0.8497109826589595 recall 0.55893536121673 f1 0.6743119266055047 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n",
      "--- EPOCH 9 ---\n",
      "Avg loss over last 500 updates: 1.1140994520187377\n",
      "Avg loss over last 500 updates: 1.32671044421196\n",
      "Avg loss over last 500 updates: 1.22110032081604\n",
      "Avg loss over last 500 updates: 1.2235952625274658\n",
      "Avg loss over last 500 updates: 1.0707467455863953\n",
      "Avg loss over last 500 updates: 1.3395685586929322\n",
      "Avg evaluation loss: 3.381626453101635\n",
      "processed 11170 tokens with 1231 phrases; found: 1154 phrases; correct: 847.\n",
      "accuracy:  73.79%; (non-O)\n",
      "accuracy:  94.37%; precision:  73.40%; recall:  68.81%; FB1:  71.03\n",
      "              LOC: precision:  88.65%; recall:  79.61%; FB1:  83.89  326\n",
      "             MISC: precision:  75.50%; recall:  59.38%; FB1:  66.47  151\n",
      "              ORG: precision:  61.84%; recall:  57.00%; FB1:  59.32  283\n",
      "              PER: precision:  68.27%; recall:  72.90%; FB1:  70.51  394\n",
      "(73.39688041594454, 68.80584890333063, 71.0272536687631)\n",
      "O precision 0.9633546225526123 recall 0.9836433611289288 f1 0.973393282200476 count 9354\n",
      "I-PER precision 0.8299845440494591 recall 0.841692789968652 f1 0.8357976653696498 count 638\n",
      "I-ORG precision 0.7620192307692307 recall 0.6469387755102041 f1 0.6997792494481235 count 490\n",
      "I-LOC precision 0.898936170212766 recall 0.8047619047619048 f1 0.8492462311557789 count 420\n",
      "I-MISC precision 0.8314606741573034 recall 0.5627376425855514 f1 0.671201814058957 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n"
     ]
    }
   ],
   "source": [
    "# Train BiLSTM Tagger Baseline\n",
    "model = BiLSTMTagger(len(word_vocab), len(label_vocab)+2, 128, 256).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model, train_data, valid_data, word_vocab, label_vocab, epochs=10, log_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freqs': Counter({'I-ORG': 2258,\n",
       "          'O': 38899,\n",
       "          'I-PER': 2544,\n",
       "          'I-LOC': 1836,\n",
       "          'I-MISC': 1011,\n",
       "          'B-MISC': 9,\n",
       "          'B-ORG': 5,\n",
       "          'B-LOC': 3}),\n",
       " 'itos': ['O',\n",
       "  'I-PER',\n",
       "  'I-ORG',\n",
       "  'I-LOC',\n",
       "  'I-MISC',\n",
       "  'B-MISC',\n",
       "  'B-ORG',\n",
       "  'B-LOC'],\n",
       " 'unk_index': None,\n",
       " 'stoi': defaultdict(None,\n",
       "             {'O': 0,\n",
       "              'I-PER': 1,\n",
       "              'I-ORG': 2,\n",
       "              'I-LOC': 3,\n",
       "              'I-MISC': 4,\n",
       "              'B-MISC': 5,\n",
       "              'B-ORG': 6,\n",
       "              'B-LOC': 7}),\n",
       " 'vectors': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vocab.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg evaluation loss: 3.389647416174412\n",
      "processed 11170 tokens with 1231 phrases; found: 1149 phrases; correct: 835.\n",
      "accuracy:  73.02%; (non-O)\n",
      "accuracy:  94.23%; precision:  72.67%; recall:  67.83%; FB1:  70.17\n",
      "              LOC: precision:  86.85%; recall:  78.24%; FB1:  82.32  327\n",
      "             MISC: precision:  76.00%; recall:  59.38%; FB1:  66.67  150\n",
      "              ORG: precision:  61.90%; recall:  55.05%; FB1:  58.28  273\n",
      "              PER: precision:  67.17%; recall:  72.63%; FB1:  69.79  399\n",
      "(72.67188859878155, 67.83103168155971, 70.16806722689076)\n",
      "O precision 0.9630443886097152 recall 0.9834295488561043 f1 0.973130223209563 count 9354\n",
      "I-PER precision 0.8140243902439024 recall 0.8369905956112853 f1 0.8253477588871716 count 638\n",
      "I-ORG precision 0.762962962962963 recall 0.6306122448979592 f1 0.6905027932960894 count 490\n",
      "I-LOC precision 0.888 recall 0.7928571428571428 f1 0.8377358490566037 count 420\n",
      "I-MISC precision 0.8287292817679558 recall 0.5703422053231939 f1 0.6756756756756757 count 263\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n"
     ]
    }
   ],
   "source": [
    "sents, true_tags, pred_tags =  evaluate(model, valid_data, word_vocab, label_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "for i,sent in enumerate(sents):\n",
    "    for j,word in enumerate(sent):\n",
    "        if word == \"<unk>\":\n",
    "            a.append(true_tags[i][j])\n",
    "            b.append(pred_tags[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_token = precision_recall_fscore_support(a,b, average=None, labels=label_vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O precision 81.71 recall 90.22 f1 85.75 count 1431\n",
      "I-PER precision 76.99 recall 79.73 f1 78.34 count 449\n",
      "I-ORG precision 62.62 recall 53.09 f1 57.46 count 243\n",
      "I-LOC precision 73.08 recall 37.62 f1 49.67 count 101\n",
      "I-MISC precision 38.89 recall 7.61 f1 12.73 count 92\n",
      "B-MISC precision 0.0 recall 0.0 f1 0.0 count 1\n",
      "B-ORG precision 0.0 recall 0.0 f1 0.0 count 0\n",
      "B-LOC precision 0.0 recall 0.0 f1 0.0 count 4\n"
     ]
    }
   ],
   "source": [
    "scores_token = precision_recall_fscore_support(a,b, average=None, labels=label_vocab.itos)\n",
    "\n",
    "for i in range(len(label_vocab.itos)):\n",
    "    print(label_vocab.itos[i] , \"precision\", round(scores_token[0][i] * 100,2), \n",
    "          \"recall\", round(scores_token[1][i] * 100, 2), \"f1\", round(scores_token[2][i] * 100, 2), \"count\", scores_token[3][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cse291_assignment2_starter_code.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
